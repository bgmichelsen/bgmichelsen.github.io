---
title: "Intro to Juce"
layout: default
date: 2024-10-05 05:30:00 -0000
categories: posts
tags: ['juce', 'dsp', 'synth']
---

# Introduction to Juce

Hello, everyone! Welcome back.

In this post I want to give a quick introduction to the Juce framework. It's the main audio library I will be using
for a good portion of my projects going forward. 

The Juce framework is an audio library built for C++. It has a lot of the audio functionality that we will need: 
working with audio buffers, generating filters, creating basic audio effects, etc. It also has a built-in GUI engine,
though I won't be going into too much detail about that. 

So, without further ado, let's get into it...

(Note: the code for the project is out on [GitHub](https://github.com/bgmichelsen/BGM01_Synth))

## The ProJucer

To begin with, Juce makes it very easy to start a project. When you download the Juce framework, it includes a tool, 
the ProJucer. This tool allows you to configure and generate the beginning source code for your project.

![ProJucer image](../_img/projucer.PNG?raw=true)

Using the ProJucer, you can select what Juce packages you want (for example, the DSP package), what type of project
it is (i.e. a standalone application or an audio plugin), and what your build environment is (i.e. Visual Studio).

For my first project, I decided to make a standalone application using Visual Studio 2022.

## Working with Audio

After the project is generated, the main entry point is the "MainComponent" class. This contains all the graphics for
the application as well as three functions for processing audio. These are as follows...

```C++
void MainComponent::prepareToPlay(int samplesPerBlock, double sampleRate)
{

}

void MainComponent::getNextAudioBlock(const juce::AudioSourceChannelInfo& bufferToFill)
{

}

void MainComponent::releaseResources()
{

}
```

The "prepareToPlay" function sets up all the information for processing audio. It gets called before any audio processing 
starts. The "sampleRate" parameter is the rate the audio will be sampled at, typically 44.1 kHz. The "samplesPerBlock" 
parameter is a bit more complicated. The way Juce processes audio is actually not at the exact sampling rate. Instead, 
it processes the audio in blocks of samples. The samples were taken at the sampling rate, but we process the block of 
samples. 

This block of samples is processed in the next function, "getNextAudioBlock." This is an audio callback that gets called 
when the next block of samples is ready to be processed. The "bufferToFill" parameter is a struct that contains three key 
variables: a "numSamples" variable, a "startSample" variable, and a "buffer." The "numSamples" is the number of samples 
in a given block, the "startSample" is the index of the sample our application should start reading from or writing to, 
and "buffer" is a pointer to the audio block the hardware is using. 

The final function, "releaseResources" gets called at the end of audio processing. It frees any resources used for 
processing audio. For example, you would free any heap allocated buffers here. 

## The Application

My application is a very rudimentary synthesizer. It adds the results of a sawtooth synthesizer to a square wave 
sub-oscillator and a noise engine. Each synth is generated by an equation for each type of synth. (Note: this is 
not the best method, see the "Improvements" section of this article). 

I also added in an adjustable low-pass IIR filter, to try and add some warmth to the resulting synth. 

The notes are generated by a built-in MIDI keyboard. The synth is monophonic, meaning it only has one voice. This 
means it can only play one note at a time: no chords for this synth! Additionally, the synth ignores any other MIDI 
information aside from the note value. It does not recognize velocity information. 

## Issues

Again, this is a very rudimentary synthesizer. As mentioned in the previous section, it ignores a lot of MIDI 
information, which would have been useful for dynamics and musical creativity. 

However, the biggest issue with this synthesizer is [aliasing](https://en.wikipedia.org/wiki/Aliasing). 
Aliasing is a phenomenon where the audio signal contains harmonics or frequency information above the 
Nyquist Frequency (i.e. half the sample rate). This causes a mirroring effect, where the program mistakes 
those higher frequencies for lower frequencies and adds them into the signal. This effect is pretty audible 
in my application. 

I tried to fix this issue using a method called [oversampling](https://en.wikipedia.org/wiki/Oversampling). 
With oversampling, you upsample the signal to a higher sampling rate, do your processing, filter out the 
frequencies above the original Nyquist Frequency, and downsample the resulting signal back into the original 
sampling rate. The easiest way to do this is to increase the size of the block by some oversampling factor 
and process that block. Then copy the main values back to a buffer of the original size.

I implemented my own naive oversampling method (as described above), though Juce does have a method of oversampling. 
My implementation of oversampling did not seem to fix the aliasing issue, unfortunately. 

## Improvements

There are a lot of improvements that can be made to this application. For one thing, I could have added in more voices 
to the synth and made better use of the MIDI data. I also could improve the UI for application, as the volume sliders 
are not labeled. 

The biggest improvement though is the synth oscillator. After researching online, it appears that most professional 
synthesizers use [wavetable synthesis](https://blog.native-instruments.com/what-is-wavetable-synthesis/). Wavetable 
synthesis makes use of tables of samples for different waveforms and goes through the samples in this table. There are 
then many methods that can be used to anti-alias the signal, such as [bandlimiting](https://ccrma.stanford.edu/~jos/resample/What_Bandlimited_Interpolation.html).

## Conclusion

Overall, this project was very rudimentary and did not result in the greatest final product. But I did end up learning a lot 
and I am now more comfortable with the Juce framework. I may end up returning to a virtual synthesizer project, possibly using 
a wavetable method instead, but for my next few projects I will be focusing on demos for some of the core DSP concepts.
